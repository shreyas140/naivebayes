{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    `Shreya Sharma`\n",
    "\n",
    "**Student ID(s):**     `910986`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_datatypes(filename, df, file_datatype):\n",
    "    '''\n",
    "    Returns a list specifying the datatype of each column, requires the filename, \n",
    "    original data and specified file datatype\n",
    "    '''\n",
    "    col=[]\n",
    "    if file_datatype=='nominal':                      #'nominal', 'ordinal' and 'numeric' have consistent column datatypes\n",
    "        col= [0 for i in range(df.shape[1]-1)]\n",
    "    elif file_datatype=='ordinal':\n",
    "        col=[1 for i in range(df.shape[1]-1)]\n",
    "    elif file_datatype=='numeric':\n",
    "        col= [2 for i in range(df.shape[1]-1)]\n",
    "    elif file_datatype=='mixed':                      #file with datatype 'mixed' need their column types hard coded\n",
    "        if filename=='adult.data':\n",
    "            col= [2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0]\n",
    "        elif filename=='bank.data':\n",
    "            col= [2, 0, 0, 1, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0]\n",
    "        elif filename=='university.data':\n",
    "            col= [0, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 0]\n",
    "    return col\n",
    "\n",
    "def discretize_numeric_data(df, col_datatypes, k_numeric_splits):\n",
    "    '''\n",
    "    PLEASE NOTE: This function was made to answer Q1\n",
    "    Transforms numeric data to discrete, requires dataframe, list of datatypes for each \n",
    "    column and the number of clusters (level of discretisation) required\n",
    "    '''\n",
    "    for i in range(len(col_datatypes)):\n",
    "        if col_datatypes[i]==2:                                          #if datatype of column is numeric\n",
    "            \n",
    "            df.iloc[:, i].fillna((df.iloc[:, i].mean()), inplace=True)   #fill empty values with average column value\n",
    "            \n",
    "            column=np.array(df.iloc[:, i]).reshape(-1,1)                 #create 2D array with column values\n",
    "            kmeans=KMeans(n_clusters=k_numeric_splits, random_state=100).fit(column) \n",
    "            centroids=kmeans.labels_                                     \n",
    "            df.iloc[:, i]=centroids                                      #replace column values with centroid labels\n",
    "            \n",
    "            col_datatypes[i]=0                                           #change datatype to nominal in datatype list\n",
    "            \n",
    "    return df, col_datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess(filename, file_datatype, header_row=None, id_column=None, class_col=-1, k_numeric_splits=None):\n",
    "    '''\n",
    "    Preprocess a .csv file to ensure consistency across data\n",
    "    \n",
    "    INPUT:\n",
    "    filename: Exact name of file\n",
    "    file_datatype: As specified in README.txt ('nominal'/'numeric'/'ordinal'/'mixed')\n",
    "    header_row: Whether there is a header in data or not\n",
    "    id_column: Whether there is an ID column in the first column of dataset\n",
    "    class_col: Index location of class labels (-1 if at end)\n",
    "    k_numeric_splits: FOR Q1 ONLY, if want to discretise all numeric data, number of centroids\n",
    "    \n",
    "    OUTPUT:\n",
    "    X: dataframe of all X attributes\n",
    "    y: array of all y class labels\n",
    "    col_datatypes: list specifying the datatypes of all columns in X\n",
    "    \n",
    "    NOTE: tokens representing missing values will be maintained in their original form\n",
    "    '''\n",
    "    \n",
    "    #Read in data as dataframe\n",
    "    df=pd.read_csv(filename, header=header_row)\n",
    "    \n",
    "    #Remove ID column if specified to exist - ID column always located in first column\n",
    "    if id_column is not None:              \n",
    "        df.drop(0, axis=1, inplace=True)\n",
    "    \n",
    "    #If class column not located as last column, move it to last column\n",
    "    if class_col != -1:\n",
    "        class_column=df[class_col]\n",
    "        del df[class_col]\n",
    "        df[class_col]=class_column\n",
    "    \n",
    "    #Find datatypes of each column\n",
    "    col_datatypes=[]    \n",
    "    col_datatypes=get_column_datatypes(filename, df, file_datatype)\n",
    "        \n",
    "    #Q1 ONLY: Discretise numeric data \n",
    "    if k_numeric_splits is not None:\n",
    "        df, col_datatypes=discretize_numeric_data(df, col_datatypes, k_numeric_splits)\n",
    "        \n",
    "    #Seperate X attributes from class by creating two dataframes\n",
    "    X=df.iloc[:, :-1]\n",
    "    y=df.iloc[:, -1]\n",
    "\n",
    "    #Re-name columns in X to their numerical order (since class column might have been moved/ID column removed)\n",
    "    X.columns = np.arange(0,len(X.columns))\n",
    "    \n",
    "    return X, y, col_datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(X, y, col_datatypes):\n",
    "    '''\n",
    "    Find the prior probabilities and likelihoods from input training data\n",
    "    \n",
    "    INPUT:\n",
    "    X: dataframe of all X attributes\n",
    "    y: array of all class labels\n",
    "    col_datatypes: list specifying datatypes of each column\n",
    "    \n",
    "    OUTPUT:\n",
    "    likelihood_prob: dictionary holding all likelihoods for input dataset\n",
    "    priors: dictionary holding prior probabilities for input dataset\n",
    "    '''\n",
    "    #n_X_attributes=number of columns in X\n",
    "    n_X_attributes=len(col_datatypes)   \n",
    "    \n",
    "    #n_rows=number of rows in X\n",
    "    n_rows=len(X)           \n",
    "    \n",
    "    #class_labels=list of unique classes (y)\n",
    "    class_labels=list(np.unique(y))           \n",
    "\n",
    "    #col_dict will contain all unique values for each non-numeric column\n",
    "    col_dict={}                               \n",
    "    for i in range(n_X_attributes):\n",
    "        if col_datatypes[i] != 2:\n",
    "            unique_vals=list(pd.unique(X.iloc[:, i])) \n",
    "            col_dict[i]=unique_vals    \n",
    "            \n",
    "    #likelihood-contains count of class lables for unique values in each non-numeric column/mu and sigma for numeric data\n",
    "    likelihood={} \n",
    "    \n",
    "    #create empty data structure first\n",
    "    for i in range(n_X_attributes):           \n",
    "        likelihood[i]={}                   \n",
    "        if col_datatypes[i]!=2:               \n",
    "            #for non-numeric data access method = likelihood[column name][unique val][class]\n",
    "            \n",
    "            for value in col_dict[i]:\n",
    "                likelihood[i][value]={}\n",
    "                for class_1 in class_labels:\n",
    "                    likelihood[i][value][class_1]=0\n",
    "        else:                                \n",
    "            #for numeric data access method = likelihood[column name][class][mu/sigma] \n",
    "            for class_1 in class_labels:\n",
    "                likelihood[i][class_1]={}\n",
    "     \n",
    "    #insert count of instances for non-numeric data into likelihood dictionary\n",
    "    for j in range(len(X)):                 \n",
    "        class_label=y.iloc[j]\n",
    "        for i in range(n_X_attributes):\n",
    "            if col_datatypes[i]!=2:\n",
    "                likelihood[i][X.iloc[j,i]][class_label]+=1\n",
    "            \n",
    "    #add class into X dataframe to assist in finding the conditional mu/sigma\n",
    "    X['y']=y                                \n",
    "    key_stats=['mu', 'sigma']\n",
    "    for i in range(n_X_attributes):\n",
    "        if col_datatypes[i]==2:   \n",
    "            #for numeric data:\n",
    "            \n",
    "            for class_1 in class_labels:\n",
    "                #insert mean into data structure:\n",
    "                likelihood[i][class_1]['mu']=X.loc[X['y'] == class_1, i].mean() \n",
    "                \n",
    "                #insert sigma into data structure\n",
    "                likelihood[i][class_1]['sigma']=X.loc[X['y'] == class_1, i].std() \n",
    "    \n",
    "    #delete class label from X dataframe, no longer required\n",
    "    del X['y']                              \n",
    "    \n",
    "    #finds the number of instances belonging to each class\n",
    "    total_class_distribution=defaultdict(int) \n",
    "    for i in range(len(X)):\n",
    "        total_class_distribution[y.iloc[i]]+=1\n",
    "        \n",
    "     #likelihood_prob - contains all likelihoods for all attributes\n",
    "    likelihood_prob={}              \n",
    "    \n",
    "    #create empty data structure, identical to likelihood\n",
    "    for i in range(n_X_attributes):         \n",
    "        likelihood_prob[i]={}\n",
    "        if col_datatypes[i]!=2:\n",
    "            for value in col_dict[i]:\n",
    "                likelihood_prob[i][value]={}\n",
    "                for class_1 in class_labels:\n",
    "                    likelihood_prob[i][value][class_1]=0\n",
    "        else:\n",
    "            #if numeric column, copy over values from likelihood\n",
    "            for class_1 in class_labels:\n",
    "                likelihood_prob[i][class_1]={}\n",
    "                for stat in likelihood[i][class_1]:\n",
    "                    likelihood_prob[i][class_1][stat]=likelihood[i][class_1][stat] \n",
    "   \n",
    "    #if non-numeric column, find the likelihood (probability value) using float division\n",
    "    for col in likelihood:                 \n",
    "        if col_datatypes[col]!=2:\n",
    "            for col_values in likelihood[col]:\n",
    "                for class_1 in total_class_distribution:\n",
    "                    likelihood_prob[col][col_values][class_1]=float(likelihood[col][col_values][class_1])/float(total_class_distribution[class_1])\n",
    "   \n",
    "    #calculate priors using total_class_distribution\n",
    "    priors={}                              \n",
    "    for class_1 in total_class_distribution:\n",
    "        priors[class_1]=float(total_class_distribution[class_1])/float(len(X))\n",
    "\n",
    "    return likelihood_prob, priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "\n",
    "def predict(X, y, col_datatypes, likelihood_prob, priors):\n",
    "    '''\n",
    "    Predict classes for items in given dataset\n",
    "    \n",
    "    INPUT:\n",
    "    X: dataframe of all X attributes\n",
    "    y: array of all class labels\n",
    "    col_datatypes: list specifying datatypes of each column\n",
    "    likelihood_prob: dictionary containing all likelihoods of all columns\n",
    "    priors: dictionary containing prior probabilities\n",
    "    \n",
    "    OUTPUT:\n",
    "    y_df: dataframe with two columns, y (recieved class labels) and y_guess (predicted class labels)\n",
    "    '''\n",
    "    \n",
    "    y_df=pd.DataFrame(y)\n",
    "    \n",
    "    #y_guess will contain all predicted classes\n",
    "    y_guess=[]      \n",
    "    \n",
    "    for row in range(len(X)):\n",
    "        #prior_x_likelihood_prob will contain result of all (prior * likelihood) values for each class\n",
    "        prior_x_likelihood_prob={}   \n",
    "        \n",
    "        for class_1 in priors:\n",
    "            #need to multiply prior value with all other likelihoods for same class\n",
    "            multiplier_temp=priors[class_1]         \n",
    "            \n",
    "            for col in range(len(X.iloc[0])):\n",
    "                if col_datatypes[col]!=2:           \n",
    "                    #if non-numeric column, access likelihood via likelihood_prob\n",
    "                    \n",
    "                    try:\n",
    "                        multiplier_temp*=likelihood_prob[col][X.iloc[row, col]][class_1]\n",
    "                    except KeyError:\n",
    "                        #if never seen instance before, assume very rare probability, epsilon\n",
    "                        multiplier_temp*=0.000001   \n",
    "               \n",
    "                else:                              \n",
    "                    #if numeric, access sigma and mu from likelihood_prob\n",
    "                    sigma=likelihood_prob[col][class_1]['sigma'] \n",
    "                    mu=likelihood_prob[col][class_1]['mu']\n",
    "                    x_hat=X.iloc[row, col]\n",
    "                    \n",
    "                    #find gaussian probability (below)\n",
    "                    multiplier_temp*=math.exp(-0.5*((x_hat-mu)/sigma)**2)*(1/(sigma*math.sqrt(2*math.pi))) \n",
    "           \n",
    "            #add (prior * likelihood) value to prior_x_likelihood_prob for each class\n",
    "            prior_x_likelihood_prob[class_1]=multiplier_temp \n",
    "        \n",
    "        #find sum of (prior * likelihood), if both probabilities 0, assume rare event, add epsilon\n",
    "        denominator=sum(prior_x_likelihood_prob.values())+0.000001 \n",
    "        \n",
    "        #divide (prior * likelihood) for each class by sum of (prior * likelihood) values for each class\n",
    "        for key in prior_x_likelihood_prob: \n",
    "            prior_x_likelihood_prob[key]=float(prior_x_likelihood_prob[key])/float(denominator) \n",
    "        \n",
    "        #find maximum probability, and associated class\n",
    "        max_value=0.0\n",
    "        max_class=''\n",
    "        for key in prior_x_likelihood_prob:\n",
    "            if float(prior_x_likelihood_prob[key])>max_value:\n",
    "                max_value=prior_x_likelihood_prob[key]\n",
    "                max_class=key\n",
    "                \n",
    "        #append associated class to y_guess\n",
    "        y_guess.append(max_class)\n",
    "        \n",
    "    #append y_guess to y_df dataframe\n",
    "    y_df['y_guess']=y_guess\n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "def evaluate(y_test_df):\n",
    "    '''\n",
    "    Evaluate the accuracy of the model by considering the number of correctly predicted labels\n",
    "    \n",
    "    INPUT:\n",
    "    y_test_df: dataframe containing 2 columns, actual class and predicted class\n",
    "    \n",
    "    OUTPUT:\n",
    "    accuracy: the accuracy value of the model\n",
    "    '''\n",
    "    correct=0\n",
    "    incorrect=0\n",
    "    for row in range(len(y_test_df)):\n",
    "        if y_test_df.iloc[row, 0]==y_test_df.iloc[row, 1]:\n",
    "            #if the prediction is equal to the actual class, then true prediction (true positive/true negative)\n",
    "            correct+=1\n",
    "        else:\n",
    "            #if prediction different from actual class, false positive/false negative\n",
    "            incorrect+=1\n",
    "    \n",
    "    #float division to find accuracy\n",
    "    accuracy=correct/float(correct+incorrect)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess() got an unexpected keyword argument 'missing_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0aa130d6ffa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_datatypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'university.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_datatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mixed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlikelihood_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_datatypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_datatypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess() got an unexpected keyword argument 'missing_value'"
     ]
    }
   ],
   "source": [
    "X, y, col_datatypes=preprocess(filename='university.data', file_datatype='mixed', missing_value=0, class_col=14)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "print(evaluate(y_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='wdbc.data', file_datatype='numeric', class_col=1)\n",
    "preprocess_datatypes=col_datatypes\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "print('Accuracy with no discretisation ' + str(evaluate(y_df)))\n",
    "accuracy=[]\n",
    "for k_split in range(1, 10):\n",
    "    X, y, col_datatypes=preprocess(filename='wdbc.data', file_datatype='numeric', class_col=1, k_numeric_splits=k_split)\n",
    "    likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "    y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "    accuracy.append(evaluate(y_df))\n",
    "    \n",
    "print('Max Accuracy with discretisation: ' + str(max(accuracy)))\n",
    "print('N K-splits to receive Max Accuracy: '+ str(accuracy.index(max(accuracy))+1))\n",
    "plt.plot(list(range(1,10)), accuracy)\n",
    "plt.xlabel('NSplits')\n",
    "plt.title('NUMERIC DATASET: WDBC Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of Numeric Attributes')\n",
    "for col in X.columns:\n",
    "    if preprocess_datatypes[col] ==2:\n",
    "        X.hist(column=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='wine.data', file_datatype='numeric', class_col=0)\n",
    "preprocess_datatypes=col_datatypes\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "print('Accuracy with no discretisation ' + str(evaluate(y_df)))\n",
    "\n",
    "accuracy=[]\n",
    "for k_split in range(1, 10):\n",
    "    X, y, col_datatypes=preprocess(filename='wine.data', file_datatype='numeric', class_col=0, k_numeric_splits=k_split)\n",
    "    likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "    y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "    accuracy.append(evaluate(y_df))\n",
    "    \n",
    "print('Max Accuracy with discretisation: ' + str(max(accuracy)))\n",
    "print('N K-splits to receive Max Accuracy: '+ str(accuracy.index(max(accuracy))+1))\n",
    "plt.plot(list(range(1,10)), accuracy)\n",
    "plt.xlabel('NSplits')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('NUMERIC DATASET: Wine Data')\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of Numeric Attributes')\n",
    "for col in X.columns:\n",
    "    if preprocess_datatypes[col] ==2:\n",
    "        X.hist(column=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='adult.data', file_datatype='mixed', missing_value='?')\n",
    "preprocess_datatypes=col_datatypes\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "print('Accuracy with no discretisation ' + str(evaluate(y_df)))\n",
    "\n",
    "accuracy=[]\n",
    "for k_split in range(1, 10):\n",
    "    X, y, col_datatypes=preprocess(filename='adult.data', file_datatype='mixed', missing_value='?', k_numeric_splits=k_split)\n",
    "    likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "    y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "    accuracy.append(evaluate(y_df))\n",
    "    \n",
    "print('Max Accuracy with discretisation: ' + str(max(accuracy)))\n",
    "print('N K-splits to receive Max Accuracy: '+ str(accuracy.index(max(accuracy))+1))\n",
    "plt.plot(list(range(1,10)), accuracy)\n",
    "plt.xlabel('NSplits')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MIXED DATASET: Adult Data')\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of Numeric Attributes')\n",
    "for col in X.columns:\n",
    "    if preprocess_datatypes[col] ==2:\n",
    "        X.hist(column=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='bank.data', file_datatype='mixed')\n",
    "preprocess_datatypes=col_datatypes\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "print('Accuracy with no discretisation ' + str(evaluate(y_df)))\n",
    "\n",
    "accuracy=[]\n",
    "for k_split in range(1, 10):\n",
    "    X, y, col_datatypes=preprocess(filename='bank.data', file_datatype='mixed', k_numeric_splits=k_split)\n",
    "    likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "    y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "    accuracy.append(evaluate(y_df))\n",
    "    \n",
    "print('Max Accuracy with discretisation: ' + str(max(accuracy)))\n",
    "print('N K-splits to receive Max Accuracy: '+ str(accuracy.index(max(accuracy))+1))\n",
    "plt.plot(list(range(1,10)), accuracy)\n",
    "plt.xlabel('NSplits')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MIXED DATASET: Bank Data')\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of Numeric Attributes')\n",
    "for col in X.columns:\n",
    "    if preprocess_datatypes[col] ==2:\n",
    "        X.hist(column=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df=pd.DataFrame()\n",
    "def zero_r(y_df):\n",
    "    y_class_count=defaultdict(int)\n",
    "    for column in y_df.columns:\n",
    "        if column != 'y_guess':\n",
    "            y_test=column\n",
    "    y_test_df=pd.DataFrame(y_df[y_test], columns=[y_test])\n",
    "    \n",
    "    y_class=list(y_test_df[y_test].unique())\n",
    "    \n",
    "    for class_1 in y_class:\n",
    "        y_class_count[class_1]=0\n",
    "\n",
    "    for row in range(len(y_df)):\n",
    "        y_class_count[y_df.iloc[row, 0]]+=1\n",
    "        \n",
    "    for (key, value) in y_class_count.items():\n",
    "        if value == max(y_class_count.values()):\n",
    "            max_class=key\n",
    "            \n",
    "    y_test_df=pd.DataFrame(y_df[y_test])\n",
    "    y_test_df['zero_r']=max_class\n",
    "    \n",
    "    return y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='breast-cancer-wisconsin.data', file_datatype='nominal', id_column=0)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Breast Cancer Wisconsin']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Breast Cancer Wisconsin Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='mushroom.data', file_datatype='nominal', missing_value='?', class_col=0)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Mushroom']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.title('Mushroom Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='lymphography.data', file_datatype='nominal', class_col=0)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Lymphography']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Lymphography Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='wdbc.data', file_datatype='numeric', class_col=1)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['WDBC']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('WDBC Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='wine.data', file_datatype='numeric', class_col=0)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "#3 classes\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Wine']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.title('Wine Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='car.data', file_datatype='ordinal')\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Car']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Car Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='nursery.data', file_datatype='ordinal')\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Nursery']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Nursery Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='somerville.data', file_datatype='ordinal', class_col=0)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Somerville']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.title('Somerville Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='adult.data', file_datatype='mixed', missing_value='?')\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Adult']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.title('Adult Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='bank.data', file_datatype='mixed')\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['Bank']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.title('Bank Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, col_datatypes=preprocess(filename='university.data', file_datatype='mixed', missing_value=0, class_col=14)\n",
    "likelihood_prob, priors=train(X, y, col_datatypes)\n",
    "y_df=predict(X, y, col_datatypes, likelihood_prob, priors)\n",
    "model_acc=evaluate(y_df)\n",
    "print('Model Accuracy: ' + str(model_acc))\n",
    "\n",
    "y_test_df=zero_r(y_df)\n",
    "zeror_acc=evaluate(y_test_df)\n",
    "print('Zero R Accuracy: ' + str(zeror_acc))\n",
    "\n",
    "performance_diff=model_acc-zeror_acc\n",
    "print('Increase in Performance using Model: ' +str(performance_diff))\n",
    "\n",
    "accuracy_df['University']=[model_acc, zeror_acc, performance_diff]\n",
    "\n",
    "performance=[model_acc, zeror_acc]\n",
    "models=['Model', 'Zero R']\n",
    "pos=[1, 1.4]\n",
    "plt.bar(pos, performance, width=0.3, align='center')\n",
    "plt.xticks(pos, models)\n",
    "plt.title('University Data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.index=['Model Accuracy', 'Zero R Accuracy', 'Difference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the na¨ıve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian na¨ıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
